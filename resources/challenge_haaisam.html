<html>
<head>
<title>EAAI Mentored Undergraduate Research Challenge: Human-Aware AI in Sound and Music</title>
<link href="../index.html" rel="index" />
<meta charset="UTF-8" />
</head>
<body>
<!-- This overall table is to adjust margins - the actual page is inside it -->
<center><table width="90%"><tr><td>

<!-- Menu that hyperlinks to the other web pages... still under construction -->

<h1>EAAI Mentored Undergraduate Research Challenge:<br />Human-Aware AI in Sound and Music</h1>
<hr />

<table><tr><td><p>Using machine learning, the <a href="https://artsandculture.google.com/experiment/blob-opera/AAHWrq360NcGbw?hl=en">Blob Opera</a> can generate opera singing and harmony.  People have been using Blob Opera to cover and/or arrange various popular songs.  If you need some background music while you read, let the AI-generated opera-singing blobs serenade you.  This playlist's songs are covered and/or arranged by <a href="https://www.youtube.com/channel/UCo1o_jCZ9QFh66QDnHvLlLw">Tom O'Connor</a>.</p></td>
    <td><iframe width="280" height="157" src="https://www.youtube-nocookie.com/embed/videoseries?list=PLES74LyU4jY125d2vYVm8R93NQbV1ffpA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td></tr></table>

<hr />

<h2>Challenge Details</h2>
<ul>
  <li>The <b>purpose of the mentored undergraduate research challenge</b> is to provide undergraduate students exposure to the <i>complete research life-cycle</i> through the guidance of a mentor familiar with the research life-cycle.  The research life-cycle includes all the steps from identifying a problem, to hypothesizing solutions, to implementation and experimentation, to ultimately reporting results in a written publication.</li>
  <li>Participating teams will submit a manuscript of their research project for peer review at the EAAI-23 Symposium, which is collocated with AAAI-23.  Teams with accepted papers will have their submission published and presented at the EAAI-23 Symposium.</li>
  <li>Research challenge teams must include:
    <ul>
      <li>At least one undergraduate (including community college) student,</li>
      <li>At least one mentor (faculty or with a Ph.D.),</li>
      <li>Anyone else, but the undergraduate student <i>must</i> be involved in the majority of the research and the mentor <i>must</i> provide regular guidance to the team.</li>
    </ul>
  <li>The <b>objective of this year's challenge</b> is to perform and publish research on human-aware AI in the application of sound and music.  The project should be doable within one semester or summer---be sure to keep the project simple and doable, addressing a single question if your problem is large.  There are many possible projects in this understudied area of research; some examples include, but are not limited to:</li>
  <ul>
    <li>Automated accompaniment (AI system plays music alongside person)</li>
    <li>Understanding a person's interests/feelings from what they play/listen to (AI system thinks about a person who performs or listens to music)</li>
    <li>Playing music based on what a person does (AI system decides what to play based on its observations of a person)</li>
    <li>Intelligent performance tutor (AI system provides feedback to a human performer to help them learn and improve)</li>
    <li>Communicating sound and music to persons who are hearing impaired (AI system adapts conveying audio information to the user)</li>
    <li>And more! Check out <a href="#projectideas">Project Ideas</a> below to get some inspiration.</li>
  </ul>
  <li>An introduction to this year's challenge can be found in the AI Matters column: "<a href="aimatters_murc23_preprint.pdf">2023 EAAI Mentored Undergraduate Research Challenge: Human-Aware AI in Sound and Music</a>"</li>
  <li><b>Timeline</b>: TBD, please check back in the future.  We will coordinate deadlines as EAAI-23 and AAAI-23 announce theirs.</li>
</ul>

<hr />

<h2>Registration</h2>
<ul>
  <li>If you have a team who is <i>interested</i> in participating, then please contact <a href="https://www.sift.net/staff/richard-freedman">Rick Freedman</a> (rfreedman at sift dot net) with:</li>
  <ul>
    <li>Team member names,</li>
    <li>Team member e-mail addresses, and </li>
    <li>Note who are the undergraduate(s) and mentor(s) on the team.</li>
  </ul>
  <li>Why register your team?</li>
  <ul>
    <li>Non-commital: registration is not a requirement to participate, but it lets the organizers know your team is considering participation.</li>
    <li>"Customer service": if your team has any questions about the challenge, then we can do our best to answer them.</li>
    <li>Updates: we can send teams updates about the challenge, including new resources, timeline changes, and deadline reminders.</li>
    <li>Program committee: to provide peer reviews to all submissions, we need to form a program committee of researchers familiar with undergraduate research.  If we can estimate the number of submissions, then we can make sure our program committee is large enough to avoid reviewing delays.  It would be appreciated, <i>but not required</i>, if team mentors are also willing to serve on the program committee and review other teams' submissions---there is <i>no conflict-of-interest</i> because this is a challenge for undergraduates to experience the complete research life-cycle, not a competition for the best research.</li>
  </ul>
</ul>

<hr />

<h2>Resources</h2>
<p>We plan to share more resources as they become available.  If you have any relevant resources that you recommend, then please send them to <a href="https://www.sift.net/staff/richard-freedman">Rick Freedman</a> (rfreedman at sift dot net) for consideration.  <i>Disclaimer: None of these resources are endorsements or advertisements.  The organizers identified these as useful materials and are sharing them for educational benefit.</i></p>

<h3>Code-Related</h3>
<ul>
  <li>We created an open-source digital modular synthesizer for this year's challenge, available on <a href="https://github.com/rgfreedman/synthesizer-eaai">GitHub</a>.  It is not required to use this code for the challenge, but it has a GUI for human interfacing (reported to an AI system) in addition to commands for an AI system to create unique sounds and play notes.  The software is written in the Processing programming language, which is built on top of the Java programming language.</li>
  <li><a href="https://github.com/plamere/spotipy">Spotipy</a> is a Python library that wraps around Spotify's Web API.  It is not required to use this code for the challenge, but it has access to licensed music, information about licensed music, and can interact with a user's Spotify account (if they have one).</li>
</ul>

<h3>Research-Related</h3>
<p>Many references are listed in the AI Matters column for this year's challenge, but additional resources about both the topic and undergraduate research are listed below:</p>
<ul>
  <li>CRA's <a href="https://conquer.cra.org/">CONQUER</a> website - Resources for faculty and undergraduate students interested in research, graduate school, and research careers in computer science.</li>
  <li>Kambhampati, Subbarao. "<a href="https://arxiv.org/abs/1910.07089">Challenges of Human-Aware AI Systems.</a>" arXiv, 2019.</li>
  <ul><li>Video presentation: <a href="https://www.youtube.com/watch?v=Hb7CWilXjag">AAAI 2018 Presidential Address</a></li></ul>
</ul>

<h3>Modular Synthesizer-Related</h3>
<p>The challenge this year provides optional code (see above) for a modular synthesizer that both humans and AI systems can use to generate music.  However, teams are not required to know how to play and/or program synthesizers to participate.  For those interested in learning about synthesizers to use them in the challenge, here are some basic resources to get started.  You can learn a lot from playing around with the code as well.</p>
<ul>
  <li><a href="https://www.youtube.com/playlist?list=PLvm7myerzlkAuECUHK9qiicbpyXQG0wlT">Basics of Modular Synthesizers - Synthesizers.com</a> YouTube playlist by <i>synthesizersdotcom</i> (10 videos, approx. 40 minutes total), to understand what various modules do.</li>
  <li><a href="https://www.youtube.com/playlist?list=PLw04ds6Q1UMfQzvhQhRdQBMXKgPTNE8_2">Eurorack for Beginners</a> YouTube playlist by <i>Noir Et Blanc Vie</i> (first 3 videos, approx. 20 minutes total), to understand selecting modules for a synthesizer.</li>
  <li><a href="https://www.youtube.com/watch?v=CKUpY0IvfS8">A Guide to Modular Synthesis with Look Mum No Computer</a> YouTube video by <i>Point Blank Music School</i> starring <i>Look Mum No Computer</i> (approx. 28 minutes), to see how it all comes together.</li>
  <li><a href="https://www.youtube.com/watch?v=yqV_uqkFTsg">What the H*ll is a Modular Synth? (Beginner's Guide)</a> YouTube video by <i>MADITRONIQUE</i> (approx. 35 minutes), where a singer/songwriter/multi-instrumentalist/producer experiments with a modular synthesizer for the first time under guidance from a bandmate.</li>
</ul>

<a name="projectideas"><h3>Project Ideas</h3></a>
<p>Far from a complete list of things a team could research, but the first step in the research life-cycle is to observe the world and come up with some questions you want to answer. Check out the videos below for some related research projects and video-inspired questions to get started brainstorming. What will your team investigate?</p>
<ul>
  <li>Shimon the robot at Georgia Tech can play music alongside human performers. What does Shimon's AI system need to understand about its fellow performers?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/l9OUbqWHOSk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>Can an AI system automatically perform <a href="https://www.youtube.com/watch?v=r12gjrtl2bI">Mickey-Mousing</a> based on someone's actions like this pianist?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/rOUbAeJfaI0?start=60" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>What can an AI system conclude about someone's mood based on the music they play or listen to?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/aWxni7NiQ6M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>How would an AI system stay in sync with a human performer? When should an AI system join in during the duet?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CF7-rz9nIn4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>How can an AI system effectively portray sound and music to individuals who have hearing impairments?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dkfCD7c2HcQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/W0pKgvoFcCA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
  <li>What can an AI system do to interpret rhythm, emotion, and other musical properties performed without sound?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2Euof4PnjDk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/X4WNDYwDGRg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
  <li>In which ways could an AI system personalize and spice up karaoke night?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/MwaOoWo0UOo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>About what would an AI system provide feedback when teaching someone an instrument and/or song?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/Ucz7t7EGXF8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/bRZ1EDf2ybU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
  <li>Watching a human act, what music should an AI system choose to play when? Flip that around: listening to a human play music, what video should an AI system choose to display when?<br /><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/NMpZrta2Cwc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
</ul>

<hr />

<h4>Organizers:</h4>
<ul>
  <li><a href="https://www.sift.net/staff/richard-freedman">Richard (Rick) G. Freedman</a>, SIFT</li>
  <li><a href="http://cs.gettysburg.edu/~tneller/index.html">Todd W. Neller</a>, Gettysburg College</li>
  <!-- and more TBA -->
</ul>

<br /><br />
<!-- Menu that hyperlinks to the other web pages... still under construction -->
<br />

</td></tr></table></center><!-- The end flags of the overall table -->
</body>
</html>

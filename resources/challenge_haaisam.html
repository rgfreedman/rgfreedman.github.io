<html>
<head>
<title>EAAI Mentored Undergraduate Research Challenge: Human-Aware AI in Sound and Music</title>
<!--<link href="../index.html" rel="index" />-->
<meta name="description" content="The purpose of the mentored undergraduate research challenge is to provide undergraduate students exposure to the complete research life-cycle through the guidance of a mentor familiar with the research life-cycle.  The objective of this year's challenge is to perform and publish research on human-aware AI in the application of sound and music." />
<meta charset="UTF-8" />
</head>
<body>
<!-- This overall table is to adjust margins - the actual page is inside it -->
<center><table width="90%"><tr><td>

<!-- Menu that hyperlinks to the other web pages... still under construction -->

<h1>EAAI Mentored Undergraduate Research Challenge:<br />Human-Aware AI in Sound and Music</h1>
<hr />

This challenge was held for EAAI 2023. <b>The new challenge for EAAI 2024 is <a href="https://www.yetanotherfreedman.com/resources/challenge_ai4aic.html">AI for Accessibility in Communication</a>.</b>

<hr />

<table><tr><td><p>Using machine learning, the <a href="https://artsandculture.google.com/experiment/blob-opera/AAHWrq360NcGbw?hl=en">Blob Opera</a> can generate opera singing and harmony.  People have been using Blob Opera to cover and/or arrange various popular songs.  If you need some background music while you read, let the AI-generated opera-singing blobs serenade you.  This playlist's songs are covered and/or arranged by <a href="https://www.youtube.com/channel/UCo1o_jCZ9QFh66QDnHvLlLw">Tom O'Connor</a>.</p></td>
    <td><iframe width="280" height="157" src="https://www.youtube-nocookie.com/embed/videoseries?list=PLES74LyU4jY125d2vYVm8R93NQbV1ffpA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></td></tr></table>

<hr />

<h2>Challenge Details</h2>
<ul>
  <li>The <b>purpose of the mentored undergraduate research challenge</b> is to provide undergraduate students exposure to the <i>complete research life-cycle</i> through the guidance of a mentor familiar with the research life-cycle.  The research life-cycle includes all the steps from identifying a problem, to hypothesizing solutions, to implementation and experimentation, to ultimately reporting results in a written publication.</li>
  <li>Participating teams will submit a manuscript of their research project for peer review at the EAAI-23 Symposium, which is collocated with AAAI-23.  Teams with accepted papers will have their submission published and presented at the EAAI-23 Symposium.</li>
  <li>Research challenge teams must include:
    <ul>
      <li>At least one undergraduate (including community college) student,</li>
      <li>At least one mentor (faculty or with a Ph.D.),</li>
      <li>Anyone else, but the undergraduate student <i>must</i> be involved in the majority of the research and the mentor <i>must</i> provide regular guidance to the team.</li>
    </ul>
  <li>The <b>objective of this year's challenge</b> is to perform and publish research on human-aware AI in the application of sound and music.  The project should be doable within one semester or summer---be sure to keep the project simple and doable, addressing a single question if your problem is large.  There are many possible projects in this understudied area of research; some examples include, but are not limited to:</li>
  <ul>
    <li>Automated accompaniment (AI system plays music alongside person)</li>
    <li>Understanding a person's interests/feelings from what they play/listen to (AI system thinks about a person who performs or listens to music)</li>
    <li>Playing music based on what a person does (AI system decides what to play based on its observations of a person)</li>
    <li>Intelligent performance tutor (AI system provides feedback to a human performer to help them learn and improve)</li>
    <li>Communicating sound and music to persons who are hearing impaired (AI system adapts conveying audio information to the user)</li>
    <li>And more! Check out <a href="#projectideas">Project Ideas</a> below to get some inspiration.</li>
  </ul>
  <li>An introduction to this year's challenge can be found in the AI Matters column: "<a href="aimatters_murc23_preprint.pdf">2023 EAAI Mentored Undergraduate Research Challenge: Human-Aware AI in Sound and Music</a>"</li>
  <li><b>Timeline</b>:</li>
  <ul>
    <li>Submission deadlines and the peer review timeline will follow those at <a href="https://pages.mtu.edu/~lebrown/eaai/">EAAI 2023</a>.  Paper submissions <i>must</i>:</li>
    <ul>
      <li>Follow the <a href="https://pages.mtu.edu/~lebrown/eaai/eaai/cfp-23.html">EAAI 2023 Submission Instructions</li>
      <li>Use the <a href="https://www.aaai.org/Publications/Templates/AnonymousSubmission23.zip">AAAI 2023 author kit</a>, and</li>
      <li>Go through EAAI 2023's <a href="https://easychair.org/conferences/?conf=eaai23">EasyChair</a> under the special track titled "EAAI Mentored Undergraduate Research Challenge 2023: Human-Aware AI in Sound and Music."</li>
    </ul>
    <li>Accepted papers will be presented at EAAI 2023 in Washington, D.C. on February 11 or 12, 2023.</li>
    <li>Specific dates:</li>
    <ul>
      <li>Submission Deadline: September 11, 2022 at 11:59 p.m. UTC-12 (anywhere on Earth)</li>
      <li>Paper Notification: November 18, 2022 at 11:59 p.m. UTC-12 (anywhere on Earth)</li>
      <li>EAAI 2023: Februrary 11 and 12, 2023</li>
    </ul>
  </ul>
</ul>

<hr />

<h2>Registration</h2>
<ul>
  <li>If you have a team who is <i>interested</i> in participating, then please contact <a href="https://www.sift.net/staff/richard-freedman">Rick Freedman</a> (rfreedman at sift dot net) with:</li>
  <ul>
    <li>Team member names,</li>
    <li>Team member e-mail addresses, and </li>
    <li>Note who are the undergraduate(s) and mentor(s) on the team.</li>
  </ul>
  <li>Why register your team?</li>
  <ul>
    <li>Non-commital: registration is not a requirement to participate, but it lets the organizers know your team is considering participation.</li>
    <li>"Customer service": if your team has any questions about the challenge, then we can do our best to answer them.</li>
    <li>Updates: we can send teams updates about the challenge, including new resources, timeline changes, and deadline reminders.</li>
    <li>Program committee: to provide peer reviews to all submissions, we need to form a program committee of researchers familiar with undergraduate research.  If we can estimate the number of submissions, then we can make sure our program committee is large enough to avoid reviewing delays.  It would be appreciated, <i>but not required</i>, if team mentors are also willing to serve on the program committee and review other teams' submissions---there is <i>no conflict-of-interest</i> because this is a challenge for undergraduates to experience the complete research life-cycle, not a competition for the best research.</li>
  </ul>
</ul>

<hr />

<h2>Resources</h2>
<p>We plan to share more resources as they become available.  If you have any relevant resources that you recommend, then please send them to <a href="https://www.sift.net/staff/richard-freedman">Rick Freedman</a> (rfreedman at sift dot net) for consideration.  <i>Disclaimer: None of these resources are endorsements or advertisements.  The organizers identified these as useful materials and are sharing them for educational benefit.</i></p>

<h3>Code-Related</h3>
<ul>
  <li>We created an open-source digital modular synthesizer for this year's challenge, available on <a href="https://github.com/rgfreedman/synthesizer-eaai">GitHub</a>.  It is not required to use this code for the challenge, but it has a GUI for human interfacing (reported to an AI system) in addition to commands for an AI system to create unique sounds and play notes.  The software is written in the Processing programming language, which is built on top of the Java programming language.</li>
  <li>A GitHub project by <i>FinFetChannel</i> provides an <a href="https://github.com/FinFetChannel/Python_Synth">open-source programmable sound generator</a> in Python.  With some modifications to the available code, one can change the waveforms (sound that it plays) and have a computer use it to play notes.</li>
  <li><a href="https://github.com/plamere/spotipy">Spotipy</a> is a Python library that wraps around Spotify's Web API.  It is not required to use this code for the challenge, but it has access to licensed music, information about licensed music, and can interact with a user's Spotify account (if they have one).</li>
</ul>

<h3>Research-Related</h3>
<p>Many references are listed in the AI Matters column for this year's challenge, but additional resources about both the topic and undergraduate research are listed below:</p>
<ul>
  <li>CRA's <a href="https://conquer.cra.org/">CONQUER</a> website - Resources for faculty and undergraduate students interested in research, graduate school, and research careers in computer science.</li>
  <li>Kambhampati, Subbarao. "<a href="https://arxiv.org/abs/1910.07089">Challenges of Human-Aware AI Systems.</a>" arXiv, 2019.</li>
  <ul><li>Video presentation: <a href="https://www.youtube.com/watch?v=Hb7CWilXjag">AAAI 2018 Presidential Address</a></li></ul>
</ul>

<h3>Modular Synthesizer-Related</h3>
<p>The challenge this year provides optional code (see above) for a modular synthesizer that both humans and AI systems can use to generate music.  However, teams are not required to know how to play and/or program synthesizers to participate.  For those interested in learning about synthesizers to use them in the challenge, here are some basic resources to get started.  You can learn a lot from playing around with the code as well.</p>
<ul>
  <li><a href="https://www.youtube.com/playlist?list=PLvm7myerzlkAuECUHK9qiicbpyXQG0wlT">Basics of Modular Synthesizers - Synthesizers.com</a> YouTube playlist by <i>synthesizersdotcom</i> (10 videos, approx. 40 minutes total), to understand what various modules do.</li>
  <li><a href="https://www.youtube.com/playlist?list=PLw04ds6Q1UMfQzvhQhRdQBMXKgPTNE8_2">Eurorack for Beginners</a> YouTube playlist by <i>Noir Et Blanc Vie</i> (first 3 videos, approx. 20 minutes total), to understand selecting modules for a synthesizer.</li>
  <li><a href="https://www.youtube.com/watch?v=CKUpY0IvfS8">A Guide to Modular Synthesis with Look Mum No Computer</a> YouTube video by <i>Point Blank Music School</i> starring <i>Look Mum No Computer</i> (approx. 28 minutes), to see how it all comes together.</li>
  <li><a href="https://www.youtube.com/watch?v=yqV_uqkFTsg">What the H*ll is a Modular Synth? (Beginner's Guide)</a> YouTube video by <i>MADITRONIQUE</i> (approx. 35 minutes), where a singer/songwriter/multi-instrumentalist/producer experiments with a modular synthesizer for the first time under guidance from a bandmate.</li>
</ul>

<a name="projectideas"><h3>Project Ideas</h3></a>
<p>Far from a complete list of things a team could research, but the first step in the research life-cycle is to observe the world and come up with some questions you want to answer. Check out the videos below for some related research projects and video-inspired questions to get started brainstorming. What will your team investigate?</p>
<ul>
  <li>Shimon the robot at Georgia Tech can play music alongside human performers. What does Shimon's AI system need to understand about its fellow performers?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/l9OUbqWHOSk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>Can an AI system automatically perform <a href="https://www.youtube.com/watch?v=r12gjrtl2bI">Mickey-Mousing</a> based on someone's actions like this pianist?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/rOUbAeJfaI0?start=60" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>What can an AI system conclude about someone's mood based on the music they play or listen to?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/aWxni7NiQ6M" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>How would an AI system stay in sync with a human performer? When should an AI system join in during the duet?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/CF7-rz9nIn4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>How can an AI system effectively portray sound and music to individuals who have hearing impairments?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/dkfCD7c2HcQ" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/W0pKgvoFcCA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
  <li>What can an AI system do to interpret rhythm, emotion, and other musical properties performed without sound?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/2Euof4PnjDk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/X4WNDYwDGRg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
  <li>In which ways could an AI system personalize and spice up karaoke night?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/MwaOoWo0UOo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><br /></li>
  <li>About what would an AI system provide feedback when teaching someone an instrument and/or song?<br /> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/Ucz7t7EGXF8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/bRZ1EDf2ybU" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
  <li>Watching a human act, what music should an AI system choose to play when? Flip that around: listening to a human play music, what video should an AI system choose to display when?<br /><iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/NMpZrta2Cwc" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></li>
</ul>

<hr />

<h2>Results</h2>
The following papers were accepted for presentation at EAAI 2023 (links to the papers coming soon):
<ul>
  <li>Emotion-Aware Music Recommendation<br /><i>Hieu Tran, Tuan Le, Anh Do, Tram Vu, Steven Bogaerts, and Brian Howard</i> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26911">link to pdf</a>]</li>
  <li>Music-to-Facial Expressions: Emotion-Based Music Visualization for the Hearing Impaired<br /><i>Yubo Wang, Fengzhou Pan, Danni Liu, and Jiaxiong Hu</i> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26912">link to pdf</a>]</li>
  <li>Predicting Perceived Music Emotions with Respect to Instrument Combinations<br /><i>Viet Dung Nguyen, Quan H. Nguyen, and Richard G. Freedman</i> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26910">link to pdf</a>]</li>
  <li>MoMusic: A Motion-Driven Human-AI Collaborative Music Composition and Performing System<br /><i>Weizhen Bian, Yijin Song, Nianzhen Gu, Tin Yan Chan, Tsz To Lo, Tsun Sun Li, King Chak Wong, Wei Xue, and Roberto Alonso Trillo</i> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26907">link to pdf</a>]</li>
  <li>Learning Adaptive Game Soundtrack Control<br /><i>Aaron Dorsey, Todd Neller, Hien Tran, and Veysel Yilmaz</i> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26909">link to pdf</a>]</li>
  <li>A Multi-User Virtual World With Music Recommendations And Mood-Based Virtual Effects<br /><i>Charats Burch, Robert Sprowl, and Mehmet Ergezer</i> [<a href="https://ojs.aaai.org/index.php/AAAI/article/view/26908">link to pdf</a>]</li>
</ul>

<hr />

<h4>Organizers:</h4>
<ul>
  <li><a href="https://www.sift.net/staff/richard-freedman">Richard (Rick) G. Freedman</a>, SIFT</li>
  <li><a href="http://cs.gettysburg.edu/~tneller/index.html">Todd W. Neller</a>, Gettysburg College</li>
  <!-- and more TBA -->
</ul>

<h4>Program Committee:</h4>
<ul>
  <li><a href="https://www.eng.ufl.edu/eed/faculty/name/ashish-aggarwal/">Ashish Aggarwal</a>, University of Florida</li>
  <li><a href="https://salfeld.github.io/">Scott Alfeld</a>, Amherst College</li>
  <li><a href="https://www.macewan.ca/academics/academic-departments/computer-science/our-people/profile/?profileid=antonc">Calin Anton</a>, Grant MacEwan University</li>
  <li><a href="https://turing.cs.hbg.psu.edu/~jjb24/">Jeremy Blum</a>, The Pennsylvania State University</li>
  <li><a href="https://www.depauw.edu/academics/college-of-liberal-arts/computer-science/faculty-staff/detail/1819876526649/">Steven Bogaerts</a>, DePauw University</li>
  <li><a href="https://www.flsouthern.edu/faculty/computer-science/eicholtz-matthew.aspx">Matthew Eicholtz</a>, Florida Southern College</li>
  <li><a href="https://wit.edu/directory/memo-ergezer">Mehmet Ergezer</a>, Wentworth Institute of Technology</li>
  <li><a href="https://cerkut.github.io/">Cumhur Erkut</a>, Aalborg University</li>
  <li><a href="http://guzdial.com/">Matthew Guzdial</a>, University of Alberta</li>
  <li><a href="https://pages.mtu.edu/~jshiebel/">Jason Hiebel</a>, Raytheon BBN Technologies and Michigan Technological University</li>
  <li><a href="https://www.khoury.northeastern.edu/people/richard-hoshino/">Richard Hoshino</a>, Northeastern University</li>
  <li><a href="http://cs.gettysburg.edu/~tneller/index.html">Todd Neller</a>, Gettysburg College</li>
  <li><a href="https://www.flsouthern.edu/faculty/computer-science/roberson-christian.aspx">Christian Roberson</a>, Florida Southern College</li>
  <li><a href="https://fandm-cares.github.io/willie.html">Jason Wilson</a>, Franklin &amp; Marshall College</li>
  <li><a href="https://sejongyoon.net/">Sejong Yoon</a>, The College of New Jersey</li>
</ul>

<h4>Past EAAI Mentored Undergraduate Research Challenge Topics:</h4>
<ul>
  <li>2022: <a href="http://cs.gettysburg.edu/~tneller/games/aiagd/index.html">AI-Assisted Game Design</a></li>
  <li>2021: <a href="http://cs.gettysburg.edu/~tneller/games/ginrummy/eaai/">Gin Rummy</a></li>
  <li>2019: <a href="http://cs.gettysburg.edu/~tneller/puzzles/boaf/">Birds of a Feather</a></li>
  <li>2017: <a href="https://www.cs.hmc.edu/~dodds/nsgc17/index.html">Widely-Accessible AI Robotics Tasks</a></li>
  <li>2015: <a href="http://cs.gettysburg.edu/~tneller/games/pokersquares/">Parameterized Poker Squares</a></li>
</ul>

<br /><br />
<!-- Menu that hyperlinks to the other web pages... still under construction -->
<br />

</td></tr></table></center><!-- The end flags of the overall table -->
</body>
</html>
